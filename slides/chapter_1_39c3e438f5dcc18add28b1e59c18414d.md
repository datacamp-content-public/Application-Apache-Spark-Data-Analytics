---
key: 39c3e438f5dcc18add28b1e59c18414d
title: 'Insert title here'
---

## Overview of what you will accomplish

```yaml
type: TitleSlide
key: 543d37a887
```

`@lower_third`
name: Full Name
title: Instructor

`@script`


---

## Overview

```yaml
type: FullCodeSlide
key: d34c9a9baf
```

`@part1`
1. pyspark.ml.feature
2. pyspark.ml.classification

`@script`
Build, fit, and evaluate a classifier model using high dimension sparse data.

We will use two Spark.ML modules. The first one for transforming the data into the form that the classifier expects. The second is a machine learning library called a classification model.

Even though the data is very sparse, because we have enough of it, it is possible to predict the class membership for many users with just a handful of data. 

To transform the text data into an array of integers we use a module from pyspark.ml.feature library called CountVectorizer. This module wants an array of strings as its input and doesn't like null values, so we need to perform some cleanup on the data to prep it for the transform step. 

To build the classifier we use a module from pyspark.ml.classification, LogisticRegression.  You will also encounter two other modules from pyspark.ml.classification, LogisticRegressionModel and LogisticRegressionSummary. 



---

## Let's practice!

```yaml
type: FinalSlide
key: 7ee0349cf9
```

`@script`
