---
title: Insert title here
key: 50a513bf7e15bc7ea8559ce3382bd96c
video_link:
  hls: https://github.com/pluteski/datacamp/blob/master/video/lesson1/DC_Lesson1_.m3u8
  mp4: https://github.com/pluteski/datacamp/blob/master/video/lesson1/DC_Lesson1_.mp4

---
## Creating and querying a SQL table in Spark

```yaml
type: "TitleSlide"
key: "253a1c93a3"
```

`@lower_third`

name: Mark Plutowski Phd
title: Algorithmic Econometrician


`@script`
Hello and welcome to this lesson about Spark SQL. In this lesson you will see how to create a SQL table from a dataframe, and then query it.  The dataframe is a fundamental data abstraction in Spark.   A Spark DataFrame is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database, also called, simply, “tabular” data. 

Tabular data comprises a data structure representing one or more rows,  also called “records”, each of which consists of a number of measurements, with each column of each row containing a single measurement. Alternatively, each row may be treated as a single observation of multiple "variables". 

"tabular" data is similar to a matrix or array, but unlike matrices, where each row and column contain the same type of data, each column in a table can be a different data type. Also unlike matrices, each column can have a name. You could have two dataframes having the same types of columns, and containing different data.  You could then concatenate the rows of data in these two tables into a single dataframe.

We said that “A Spark DataFrame is a distributed collection of data organized into named columns.”  What do we mean by “distributed” ?  Spark can split this dataset into parts (6 fast clicks), then store each part on different servers. In this case, Spark is partitioning the data and distributing it automatically, on your behalf. 

This is one technique that Spark uses to handle large datasets, even though each server may not have enough storage to hold the entire dataset on its own.  What’s more, Spark allows you to treat a dataframe like a table, and query it using SQL. 

Now, What is SQL?  SQL stands for “Structured Query Language”, and is a common way of fetching data from one or more tables. This fetching process is called “querying” and the statement you use to tell the computer what to fetch is called a “query”. 

What’s useful about the Spark SQL table is that it allows you to take the data that is in a dataframe -- namely, a distributed collection of rows having named columns -- and treat it as a single table, and fetch data from it using an SQL query. 

Let’s load some data into a dataframe, convert it into a sql table, and query it.


---
## Spark SQL

```yaml
type: "FullImageSlide"
key: "d63bac7ed7"
```

`@part1`
![Spark SQL](https://github.com/pluteski/datacamp/blob/master/images/lesson1_images/Spark%20SQL.png)


`@script`



---
## Let's try out SQL tables

```yaml
type: "FinalSlide"
key: "4695e78972"
```

`@script`


