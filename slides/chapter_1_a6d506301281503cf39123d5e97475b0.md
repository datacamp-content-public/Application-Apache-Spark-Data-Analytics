---
title: Insert title here
key: a6d506301281503cf39123d5e97475b0

---
## Dot notation vs SQL

```yaml
type: "TitleSlide"
key: "0316a3adb2"
```

`@lower_third`

name: Mark E Plutowski
title: Algorithmic Econometrician


`@script`
Hello and welcome to this lesson about Spark SQL. In this lesson you will see how you can perform SQL operations using either SQL queries or using standard dataframe dot notation. 

Pyspark often gives you two or more ways to accomplish the same task.


---


```yaml
type: "FullCodeSlide"
key: "c277d712e8"
center_content: false
```

`@part1`
```
>>> df
DataFrame[train_id: string, station: string, time: string]
```

```
>>> df.columns
['train_id', 'station', 'time']
```


```
>>> df.show(5)
+--------+-------------+-----+
|train_id|      station| time|
+--------+-------------+-----+
|     324|San Francisco|7:59a|
|     324|  22nd Street|8:03a|
|     324|     Millbrae|8:16a|
|     324|    Hillsdale|8:24a|
|     324| Redwood City|8:31a|
+--------+-------------+-----+
```


`@script`
For example, suppose you have a dataframe containing three columns. and you want to select two columns...


---
## Select two columns

```yaml
type: "FullCodeSlide"
key: "5db7edc78c"
```

`@part1`
```
df.select('train_id','station')
  .show(5)

+--------+-------------+
|train_id|      station|
+--------+-------------+
|     324|San Francisco|
|     324|  22nd Street|
|     324|     Millbrae|
|     324|    Hillsdale|
|     324| Redwood City|
+--------+-------------+
```


`@script`
You could do this.


---
## Select two columns

```yaml
type: "FullCodeSlide"
key: "be6b045502"
```

`@part1`
- df.select('**train_id**','station')
- df.select(**df.train_id**,df.station) {{1}}
- from pyspark.sql.functions import **col** {{2}} 
- df.select(**col**('train_id'), col('station')) {{3}}


`@script`
See that the column ‘train_id’ is a string given in quotes. 

You can also do this {{1}}, using dot notation.  This time the column is given in dot notation, as df.train_id.

You can also import this function {{2}}, which allows you to do this {{3}}.  This time, the name of the column is given as an argument to this new operator called col.


---
## Renaming a column

```yaml
type: "FullCodeSlide"
key: "6ee278e5ed"
```

`@part1`
```
df.select('train_id','station')
  .withColumnRenamed('train_id','train')
  .show(5)
```

```
+-----+-------------+
|train|      station|
+-----+-------------+
|  324|San Francisco|
|  324|  22nd Street|
|  324|     Millbrae|
|  324|    Hillsdale|
|  324| Redwood City|
+-----+-------------+
```

```
df.select(col('train_id').alias('train'), 'station') 
```
{{1}}


`@script`
For example, to rename a column you can use the withColumnRenamed function.  But you could also use the col operator, like so {{1}}.  This is often handy.


---
## No!

```yaml
type: "FullSlide"
key: "2e407f5bff"
center_content: true
```

`@part1`
df.select('train_id',  df.station,  col('time'))


`@script`
Just try not to use all three conventions at the same time without good reason.


---
## SQL queries using dot notation

```yaml
type: "FullCodeSlide"
key: "4d771bfeee"
```

`@part1`
```
spark.sql('select train_id as train, station from df limit 5')
     .show()
```

```
+-----+-------------+
|train|      station|
+-----+-------------+
|  324|San Francisco|
|  324|  22nd Street|
|  324|     Millbrae|
|  324|    Hillsdale|
|  324| Redwood City|
+-----+-------------+
```

```
df.select(col('train_id').alias('train'), 'station')
  .limit(5)
  .show()
```
{{1}}


`@script`
Most Spark sql queries can be done in dot notation or sql notation.  Here’s an example.  Notice that the limit operation is done at query time instead of at show time.  We can do this exact same query using dot notation, like so {{1}}, giving the same result. Note how we used the col operator to select the train_id column, so that we could rename it in place.


---
## Insert title here...

```yaml
type: "FullCodeSlide"
key: "d3ee8f4271"
```

`@part1`



`@script`



---
## Final Slide

```yaml
type: "FinalSlide"
key: "9628eb004d"
```

`@script`


